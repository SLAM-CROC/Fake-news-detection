{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e52d71bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4366a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(filename):\n",
    "    features = []\n",
    "    labels = []\n",
    "    count = 1\n",
    "    blank_data_number_list = []\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for line in reader:\n",
    "            # print(line[1:3])\n",
    "            if line[1] == 'title':\n",
    "                count += 1\n",
    "                continue\n",
    "            elif line[1] == ' ' or line[2] == ' ':\n",
    "                blank_data_number_list.append(count)\n",
    "                count += 1\n",
    "                continue\n",
    "            features.append(line[1:3])\n",
    "            labels.append(line[3])\n",
    "            count += 1\n",
    "    print(\"Loaded csv file, and there are \" + str(len(blank_data_number_list)) + \" blank text have been removed \"\n",
    "                                                                                 \"from dataset\\n\" + str(len(features)) +\n",
    "          \" data points in total\")\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "351c29f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded csv file, and there are 36 blank text have been removed from dataset\n",
      "6299 data points in total\n"
     ]
    }
   ],
   "source": [
    "features_data, labels_data = load_csv(\"./news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b34a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(features):\n",
    "    count = 0\n",
    "    for i in features:\n",
    "        if 'http://' in i[1] or 'https://' in i[1]:\n",
    "            count += 1\n",
    "            i[1] = re.sub(r'http\\S+', ' ', i[1])\n",
    "    print(\"There are \"+str(count)+\" url have been removed from text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6cce1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 295 url have been removed from text\n"
     ]
    }
   ],
   "source": [
    "remove_url(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26ec4edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newline(features):\n",
    "    for i in features:\n",
    "        i[1] = i[1].replace('\\n', ' ').replace('\\r', ' ').replace('\\n\\n', ' ')\n",
    "    print(\"Newline symbols have been removed from text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0ef479e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newline symbols have been removed from text\n"
     ]
    }
   ],
   "source": [
    "remove_newline(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc44b9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_number(features):\n",
    "    for i in features:\n",
    "        i[1] = re.sub(r'\\d+', ' ', i[1])\n",
    "    print(\"Numbers have been removed from text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7aa3974d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers have been removed from text\n"
     ]
    }
   ],
   "source": [
    "remove_number(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "967eeb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(features):\n",
    "    for i in features:\n",
    "        i[1] = re.sub('[^a-zA-Z]', ' ', i[1])\n",
    "    print(\"Punctuations have been removed from text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c3ceef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuations have been removed from text\n"
     ]
    }
   ],
   "source": [
    "remove_punctuation(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0195c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_into_lowercase(features):\n",
    "    for i in features:\n",
    "        i[1] = i[1].lower()\n",
    "    print(\"All text have been converted into lowercase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b54a9300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All text have been converted into lowercase\n"
     ]
    }
   ],
   "source": [
    "convert_into_lowercase(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bb12697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(features):\n",
    "    for i in features:\n",
    "        i[1] = word_tokenize(i[1])\n",
    "    print(\"Preformed tokenization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84ef1bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preformed tokenization\n"
     ]
    }
   ],
   "source": [
    "tokenization(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dbc1976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(features):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for i in features:\n",
    "        i[1] = [words for words in i[1] if not words in stop_words]\n",
    "    print(\"Stopwords have been removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0371d537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords have been removed\n"
     ]
    }
   ],
   "source": [
    "remove_stopwords(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f6c0a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(features):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemma = WordNetLemmatizer()\n",
    "    for i in features:\n",
    "        i[1] = [stemmer.stem(word) for word in i[1]]\n",
    "    for i in features:\n",
    "        i[1] = [lemma.lemmatize(word=word, pos='v') for word in i[1]]\n",
    "    print(\"Text has been normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eca661ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text has been normalized\n"
     ]
    }
   ],
   "source": [
    "normalization(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81d5e983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_words(features):\n",
    "    for i in features:\n",
    "        i[1] = [word for word in i[1] if len(word) > 2]\n",
    "    print(\"Short words have been removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b3fcf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short words have been removed\n"
     ]
    }
   ],
   "source": [
    "remove_short_words(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfa815cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_to_string(features):\n",
    "    for i in features:\n",
    "        i[1] = ' '.join(i[1])\n",
    "    print('The text have been recovered from words to string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09bba08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text have been recovered from words to string\n"
     ]
    }
   ],
   "source": [
    "recover_to_string(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50b011ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(texts):\n",
    "    for i in texts:\n",
    "        tokenizer = Tokenizer(filters=' ')  # set num_words to default None, process all words\n",
    "        tokenizer.fit_on_texts([i[1]])\n",
    "        i[1] = tokenizer.texts_to_sequences([i[1]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7adf14d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6639d275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9881"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_max_sequence_len(features):\n",
    "    max = 0\n",
    "    for i in features:\n",
    "        if len(i[1]) > max:\n",
    "            max = len(i[1])\n",
    "        else:\n",
    "            continue\n",
    "    return max\n",
    "\n",
    "MAX_SEQ_LEN = find_max_sequence_len(features_data)\n",
    "MAX_SEQ_LEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ef8aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(features):\n",
    "    for i in features:\n",
    "#         print(i[1])\n",
    "        i[1] = pad_sequences([i[1]], maxlen = 1000)\n",
    "\n",
    "padding(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4235105",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_data[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3af74395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(labels_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "66ab79d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_data = np.array(labels_data)\n",
    "labels_data = LabelEncoder().fit_transform(labels_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3666d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features, labels, test_proportion):\n",
    "    index = int(len(features) * (1 - test_proportion))\n",
    "    train_x, train_y = np.array(features[:index],dtype=object), np.array(labels[:index],dtype=object)\n",
    "    test_x, test_y = np.array(features[index:],dtype=object), np.array(labels[index:],dtype=object)\n",
    "    return (train_x, train_y), (test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac97c853",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = split_data(features_data, labels_data, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "76a5c087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000)\n",
      "(5039,)\n",
      "(1260, 2)\n",
      "(1260,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_train[0][1].shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "405139c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 1], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.7",
   "language": "python",
   "name": "pythonproject1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
