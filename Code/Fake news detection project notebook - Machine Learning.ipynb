{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ebab24",
   "metadata": {},
   "source": [
    "# Import NLTK, TensorFlow, Keras Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e52d71bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217563ca",
   "metadata": {},
   "source": [
    "# Define Data Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4366a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(filename):\n",
    "    features = []\n",
    "    labels = []\n",
    "    count = 1\n",
    "    blank_data_number_list = []\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for line in reader:\n",
    "            # print(line[1:3])\n",
    "            if line[1] == 'title':\n",
    "                count += 1\n",
    "                continue\n",
    "            elif line[1] == ' ' or line[2] == ' ':\n",
    "                blank_data_number_list.append(count)\n",
    "                count += 1\n",
    "                continue\n",
    "            features.append(line[1:3])\n",
    "            labels.append(line[3])\n",
    "            count += 1\n",
    "    print(\"Loaded csv file, and there are \" + str(len(blank_data_number_list)) + \" blank text have been removed \"\n",
    "                                                                                 \"from dataset\\n\" + str(len(features)) +\n",
    "          \" data points in total\")\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b34a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(features):\n",
    "    count = 0\n",
    "    for i in features:\n",
    "        if 'http://' in i[1] or 'https://' in i[1]:\n",
    "            count += 1\n",
    "            i[1] = re.sub(r'http\\S+', ' ', i[1])\n",
    "    print(\"There are \"+str(count)+\" url have been removed from text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26ec4edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newline(features):\n",
    "    for i in features:\n",
    "        i[1] = i[1].replace('\\n', ' ').replace('\\r', ' ').replace('\\n\\n', ' ')\n",
    "    print(\"Newline symbols have been removed from text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc44b9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_number(features):\n",
    "    for i in features:\n",
    "        i[1] = re.sub(r'\\d+', ' ', i[1])\n",
    "    print(\"Numbers have been removed from text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "967eeb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(features):\n",
    "    for i in features:\n",
    "        i[1] = re.sub('[^a-zA-Z]', ' ', i[1])\n",
    "    print(\"Punctuations have been removed from text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0195c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_into_lowercase(features):\n",
    "    for i in features:\n",
    "        i[1] = i[1].lower()\n",
    "    print(\"All text have been converted into lowercase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bb12697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(features):\n",
    "    for i in features:\n",
    "        i[1] = word_tokenize(i[1])\n",
    "    print(\"Preformed tokenization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dbc1976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(features):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for i in features:\n",
    "        i[1] = [words for words in i[1] if not words in stop_words]\n",
    "    print(\"Stopwords have been removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f6c0a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(features):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemma = WordNetLemmatizer()\n",
    "    for i in features:\n",
    "        i[1] = [stemmer.stem(word) for word in i[1]]\n",
    "    for i in features:\n",
    "        i[1] = [lemma.lemmatize(word=word, pos='v') for word in i[1]]\n",
    "    print(\"Text has been normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81d5e983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_words(features):\n",
    "    for i in features:\n",
    "        i[1] = [word for word in i[1] if len(word) > 2]\n",
    "    print(\"Short words have been removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "630d69f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_to_string(features):\n",
    "    for i in features:\n",
    "        i[1] = ' '.join(i[1])\n",
    "    print('The text have been recovered from words to string')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3a6d8e",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "351c29f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded csv file, and there are 36 blank text have been removed from dataset\n",
      "6299 data points in total\n"
     ]
    }
   ],
   "source": [
    "features_data, labels_data = load_csv(\"./news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6cce1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 295 url have been removed from text\n"
     ]
    }
   ],
   "source": [
    "remove_url(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0ef479e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newline symbols have been removed from text\n"
     ]
    }
   ],
   "source": [
    "remove_newline(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7aa3974d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers have been removed from text\n"
     ]
    }
   ],
   "source": [
    "remove_number(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c3ceef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuations have been removed from text\n"
     ]
    }
   ],
   "source": [
    "remove_punctuation(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b54a9300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All text have been converted into lowercase\n"
     ]
    }
   ],
   "source": [
    "convert_into_lowercase(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84ef1bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preformed tokenization\n"
     ]
    }
   ],
   "source": [
    "tokenization(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0371d537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords have been removed\n"
     ]
    }
   ],
   "source": [
    "remove_stopwords(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eca661ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text has been normalized\n"
     ]
    }
   ],
   "source": [
    "normalization(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b3fcf7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short words have been removed\n"
     ]
    }
   ],
   "source": [
    "remove_short_words(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bc0deca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text have been recovered from words to string\n"
     ]
    }
   ],
   "source": [
    "recover_to_string(features_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9070d7ee",
   "metadata": {},
   "source": [
    "# Drop News Titile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c4dcb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(features_data):\n",
    "    extracted = []\n",
    "    for i in features_data:\n",
    "        extracted.append(i[1])\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40ce4861",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data = extract_text(features_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c67f1f",
   "metadata": {},
   "source": [
    "# Encode Lable & Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "554edfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels):\n",
    "    labels = np.array(labels)\n",
    "    labels = LabelEncoder().fit_transform(labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61329062",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_data = encode_labels(labels_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea82f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features, labels, test_proportion):\n",
    "    index = int(len(features) * (1 - test_proportion))\n",
    "    train_x, train_y = np.array(features[:index],dtype=object), np.array(labels[:index],dtype=object)\n",
    "    test_x, test_y = np.array(features[index:],dtype=object), np.array(labels[index:],dtype=object)\n",
    "    return (train_x, train_y), (test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adc0ad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = split_data(features_data, labels_data, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0059ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.astype('int')\n",
    "y_train = y_train.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ba45758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5669,)\n",
      "(5669,)\n",
      "(630,)\n",
      "(630,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf2ed6a",
   "metadata": {},
   "source": [
    "# Training Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1dee33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2d3e889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8333333333333334\n",
      "F1 Score:  0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "pipe1 = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('model', MultinomialNB())])\n",
    "model1 = pipe1.fit(x_train, y_train)\n",
    "result1 =  model1.predict(x_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, result1))\n",
    "print(\"F1 Score: \", f1_score(y_test, result1, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf285db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9317460317460318\n",
      "F1 Score:  0.9317460317460318\n"
     ]
    }
   ],
   "source": [
    "pipe2 = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('model', LogisticRegressionCV(cv=5, scoring='accuracy', random_state=0, n_jobs=-1,  max_iter=300))])\n",
    "model2 = pipe2.fit(x_train, y_train)\n",
    "result2 =  model2.predict(x_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, result2))\n",
    "print(\"F1 Score: \", f1_score(y_test, result2, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be5bcadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8698412698412699\n",
      "F1 Score:  0.8698412698412697\n"
     ]
    }
   ],
   "source": [
    "pipe3 = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('model', KNeighborsClassifier(n_neighbors=5))])\n",
    "model3 = pipe3.fit(x_train, y_train)\n",
    "result3 =  model3.predict(x_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, result3))\n",
    "print(\"F1 Score: \", f1_score(y_test, result3, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "94b0f067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9333333333333333\n",
      "F1 Score:  0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "pipe4 = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('model', SVC(kernel='linear', random_state=1))])\n",
    "model4 = pipe4.fit(x_train, y_train)\n",
    "result4 =  model4.predict(x_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, result4))\n",
    "print(\"F1 Score: \", f1_score(y_test, result4, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ecaf62ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7904761904761904\n",
      "F1 Score:  0.7904761904761904\n"
     ]
    }
   ],
   "source": [
    "pipe5 = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('model', DecisionTreeClassifier())])\n",
    "model5 = pipe5.fit(x_train, y_train)\n",
    "result5 =  model5.predict(x_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, result5))\n",
    "print(\"F1 Score: \", f1_score(y_test, result5, average='micro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.7",
   "language": "python",
   "name": "pythonproject1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
