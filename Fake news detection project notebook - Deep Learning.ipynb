{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ebab24",
   "metadata": {},
   "source": [
    "# Import NLTK, TensorFlow, Keras Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e52d71bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217563ca",
   "metadata": {},
   "source": [
    "# Define Data Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4366a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(filename):\n",
    "    features = []\n",
    "    labels = []\n",
    "    count = 1\n",
    "    blank_data_number_list = []\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for line in reader:\n",
    "            # print(line[1:3])\n",
    "            if line[1] == 'title':\n",
    "                count += 1\n",
    "                continue\n",
    "            elif line[1] == ' ' or line[2] == ' ':\n",
    "                blank_data_number_list.append(count)\n",
    "                count += 1\n",
    "                continue\n",
    "            features.append(line[1:3])\n",
    "            labels.append(line[3])\n",
    "            count += 1\n",
    "    print(\"Loaded csv file, and there are \" + str(len(blank_data_number_list)) + \" blank text have been removed \"\n",
    "                                                                                 \"from dataset\\n\" + str(len(features)) +\n",
    "          \" data points in total\")\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b34a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(features):\n",
    "    count = 0\n",
    "    for i in features:\n",
    "        if 'http://' in i[1] or 'https://' in i[1]:\n",
    "            count += 1\n",
    "            i[1] = re.sub(r'http\\S+', ' ', i[1])\n",
    "    print(\"There are \"+str(count)+\" url have been removed from text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26ec4edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newline(features):\n",
    "    for i in features:\n",
    "        i[1] = i[1].replace('\\n', ' ').replace('\\r', ' ').replace('\\n\\n', ' ')\n",
    "    print(\"Newline symbols have been removed from text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc44b9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_number(features):\n",
    "    for i in features:\n",
    "        i[1] = re.sub(r'\\d+', ' ', i[1])\n",
    "    print(\"Numbers have been removed from text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "967eeb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(features):\n",
    "    for i in features:\n",
    "        i[1] = re.sub('[^a-zA-Z]', ' ', i[1])\n",
    "    print(\"Punctuations have been removed from text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0195c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_into_lowercase(features):\n",
    "    for i in features:\n",
    "        i[1] = i[1].lower()\n",
    "    print(\"All text have been converted into lowercase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bb12697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(features):\n",
    "    for i in features:\n",
    "        i[1] = word_tokenize(i[1])\n",
    "    print(\"Preformed tokenization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dbc1976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(features):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for i in features:\n",
    "        i[1] = [words for words in i[1] if not words in stop_words]\n",
    "    print(\"Stopwords have been removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f6c0a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(features):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemma = WordNetLemmatizer()\n",
    "    for i in features:\n",
    "        i[1] = [stemmer.stem(word) for word in i[1]]\n",
    "    for i in features:\n",
    "        i[1] = [lemma.lemmatize(word=word, pos='v') for word in i[1]]\n",
    "    print(\"Text has been normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81d5e983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_words(features):\n",
    "    for i in features:\n",
    "        i[1] = [word for word in i[1] if len(word) > 2]\n",
    "    print(\"Short words have been removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3a6d8e",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "351c29f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded csv file, and there are 36 blank text have been removed from dataset\n",
      "6299 data points in total\n"
     ]
    }
   ],
   "source": [
    "features_data, labels_data = load_csv(\"./news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6cce1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 295 url have been removed from text\n"
     ]
    }
   ],
   "source": [
    "remove_url(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0ef479e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newline symbols have been removed from text\n"
     ]
    }
   ],
   "source": [
    "remove_newline(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7aa3974d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers have been removed from text\n"
     ]
    }
   ],
   "source": [
    "remove_number(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c3ceef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuations have been removed from text\n"
     ]
    }
   ],
   "source": [
    "remove_punctuation(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b54a9300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All text have been converted into lowercase\n"
     ]
    }
   ],
   "source": [
    "convert_into_lowercase(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84ef1bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preformed tokenization\n"
     ]
    }
   ],
   "source": [
    "tokenization(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0371d537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords have been removed\n"
     ]
    }
   ],
   "source": [
    "remove_stopwords(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eca661ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text has been normalized\n"
     ]
    }
   ],
   "source": [
    "normalization(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b3fcf7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short words have been removed\n"
     ]
    }
   ],
   "source": [
    "remove_short_words(features_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3032ee",
   "metadata": {},
   "source": [
    "# Drop News Tittle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75d79037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(features_data):\n",
    "    extracted = []\n",
    "    for i in features_data:\n",
    "        extracted.append(i[1])\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35334711",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data = extract_text(features_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc67ed7",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ff3f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a185fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimension of vectors we are generating\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "#Creating Word Vectors by Word2Vec Method\n",
    "w2v_model = gensim.models.Word2Vec(sentences=features_data, size=EMBEDDING_DIM, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36696f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42551"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1187848c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhaosiqi/.conda/envs/pythonProject1/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 4.39992994e-01,  5.50618721e-03,  8.79078567e-01,  1.40246987e+00,\n",
       "       -3.49762142e-01, -2.33372480e-01, -3.00124556e-01,  1.27946293e+00,\n",
       "        1.06896675e+00, -5.22127096e-03,  2.36849025e-01, -3.66457009e+00,\n",
       "       -1.11039209e+00, -2.80872613e-01,  6.50587678e-01, -1.98138714e-01,\n",
       "        5.99166341e-02, -4.01443750e-01, -6.91692114e-01,  8.82939100e-01,\n",
       "        1.09929895e+00, -1.86785683e-01, -7.29182482e-01, -6.98627606e-02,\n",
       "        1.82557344e-01,  9.33465660e-01, -1.05538213e+00, -2.43873224e-01,\n",
       "        2.81094581e-01,  1.73928487e+00,  4.72934060e-02, -8.25273991e-02,\n",
       "        4.65616763e-01,  1.03177726e+00, -2.97019422e-01,  1.63644738e-02,\n",
       "        1.82274848e-01, -8.57055247e-01, -1.12198555e+00,  5.51674366e-01,\n",
       "       -8.36056888e-01, -5.25856912e-01,  1.34128064e-01,  1.05803758e-01,\n",
       "       -7.36937374e-02,  2.79062897e-01,  1.70134902e+00, -1.55509090e+00,\n",
       "       -1.95021808e+00, -5.39728880e-01, -1.70033261e-01, -7.97155321e-01,\n",
       "        1.12898624e+00,  6.37694538e-01,  6.95465922e-01, -1.64449140e-02,\n",
       "       -7.13586926e-01, -2.44051456e+00, -8.34103048e-01, -1.07094240e+00,\n",
       "        3.16606224e-01, -1.51096389e-01, -9.53357935e-01,  7.98200786e-01,\n",
       "       -1.11730434e-01, -9.38015163e-01,  1.45012987e+00, -5.17032027e-01,\n",
       "       -2.91696221e-01,  6.34437919e-01, -6.15517199e-01,  2.00767708e+00,\n",
       "        5.23551181e-02, -5.87771058e-01,  6.89771056e-01,  1.77252293e-03,\n",
       "       -4.73042503e-02,  2.77464181e-01,  1.31115806e+00, -1.99347472e+00,\n",
       "       -8.80749404e-01, -1.53826821e+00, -2.55900717e+00, -1.52419016e-01,\n",
       "        8.01965654e-01,  5.17397106e-01,  1.00852184e-01,  5.37263751e-01,\n",
       "        1.06398821e+00, -7.39725903e-02,  8.67713988e-01, -1.76094913e+00,\n",
       "        2.19832826e+00, -1.40011430e+00,  1.03770864e+00, -9.17844951e-01,\n",
       "        6.30619287e-01,  1.83349264e+00, -2.35199809e-01, -1.16007641e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model[\"say\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3031f46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tell', 0.6692366600036621),\n",
       " ('ask', 0.5896276235580444),\n",
       " ('agre', 0.5827974081039429),\n",
       " ('respond', 0.5325295925140381),\n",
       " ('believ', 0.5156740546226501),\n",
       " ('speak', 0.5016465187072754),\n",
       " ('insist', 0.4966651201248169),\n",
       " ('acknowledg', 0.48986852169036865),\n",
       " ('suggest', 0.48980599641799927),\n",
       " ('admit', 0.48370590806007385)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"say\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa952316",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bad', 0.8332777619361877),\n",
       " ('terribl', 0.675784707069397),\n",
       " ('happi', 0.6739716529846191),\n",
       " ('better', 0.6647763252258301),\n",
       " ('hurt', 0.6627670526504517),\n",
       " ('unobtain', 0.6375031471252441),\n",
       " ('faustian', 0.6356115937232971),\n",
       " ('obvious', 0.6322229504585266),\n",
       " ('mayb', 0.621401309967041),\n",
       " ('nautil', 0.6210019588470459)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "863bd5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create weight matrix from word2vec gensim model\n",
    "def get_weight_matrix(model, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        weight_matrix[i] = model[word]\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8acdbf",
   "metadata": {},
   "source": [
    "# Unique Numbers Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d34aef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(extracted):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(extracted)\n",
    "    extracted = tokenizer.texts_to_sequences(extracted)\n",
    "    return extracted, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7fba7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data, tokenizer = vectorize(features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf631d78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhaosiqi/.conda/envs/pythonProject1/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "embedding_vectors = get_weight_matrix(w2v_model, tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8d37e1",
   "metadata": {},
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30687a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist([len(x) for x in features_data], bins=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51066025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5921"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nos = np.array([len(x) for x in features_data])\n",
    "len(nos[nos  < 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c81ad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 1000\n",
    "features_data = pad_sequences(features_data, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859e65d1",
   "metadata": {},
   "source": [
    "# Encode Labels & Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf947ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels):\n",
    "    labels = np.array(labels)\n",
    "    labels = LabelEncoder().fit_transform(labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cdf90822",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_data = encode_labels(labels_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "623e3c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features, labels, test_proportion):\n",
    "    index = int(len(features) * (1 - test_proportion))\n",
    "    train_x, train_y = np.array(features[:index],dtype=object), np.array(labels[:index],dtype=object)\n",
    "    test_x, test_y = np.array(features[index:],dtype=object), np.array(labels[index:],dtype=object)\n",
    "    return (train_x, train_y), (test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0525efc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = split_data(features_data, labels_data, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78e30a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5669, 1000)\n",
      "(5669,)\n",
      "(630, 1000)\n",
      "(630,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f2ce2",
   "metadata": {},
   "source": [
    "# Training Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fbfac8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, MaxPool1D, Conv1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import BatchNormalization\n",
    "import tensorflow as tf\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd89a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f83eb35",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "189f3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c9287ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-11 15:54:49.873337: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-11 15:54:49.873832: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1000, 100)         4255200   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 4,372,577\n",
      "Trainable params: 117,377\n",
      "Non-trainable params: 4,255,200\n",
      "_________________________________________________________________\n",
      "Train on 4535 samples, validate on 1134 samples\n",
      "Epoch 1/6\n",
      "4535/4535 [==============================] - 445s 98ms/step - loss: 0.5422 - accuracy: 0.7336 - val_loss: 0.5505 - val_accuracy: 0.7178\n",
      "Epoch 2/6\n",
      "4535/4535 [==============================] - 473s 104ms/step - loss: 0.4344 - accuracy: 0.7971 - val_loss: 0.4980 - val_accuracy: 0.7716\n",
      "Epoch 3/6\n",
      "4535/4535 [==============================] - 464s 102ms/step - loss: 0.3681 - accuracy: 0.8419 - val_loss: 0.4608 - val_accuracy: 0.7972\n",
      "Epoch 4/6\n",
      "4535/4535 [==============================] - 459s 101ms/step - loss: 0.3378 - accuracy: 0.8606 - val_loss: 0.3826 - val_accuracy: 0.8474\n",
      "Epoch 5/6\n",
      "4535/4535 [==============================] - 428s 94ms/step - loss: 0.3144 - accuracy: 0.8719 - val_loss: 0.3875 - val_accuracy: 0.8289\n",
      "Epoch 6/6\n",
      "4535/4535 [==============================] - 892s 197ms/step - loss: 0.3298 - accuracy: 0.8591 - val_loss: 0.3439 - val_accuracy: 0.8554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb0204385d0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Defining Neural Network\n",
    "model = Sequential()\n",
    "#Non-trainable embeddidng layer\n",
    "model.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=maxlen, trainable=False))\n",
    "#LSTM \n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, validation_split=0.2, epochs=6, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a1d52c62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.8698412775993347\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31fb2100",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04953572154045105"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = model.predict(x_train[0:1])\n",
    "float(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606d9519",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d81119a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1000, 100)         4255200   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000, 100)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 100000)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               51200512  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 55,456,225\n",
      "Trainable params: 51,201,025\n",
      "Non-trainable params: 4,255,200\n",
      "_________________________________________________________________\n",
      "Train on 4535 samples, validate on 1134 samples\n",
      "Epoch 1/6\n",
      "4535/4535 [==============================] - 82s 18ms/step - loss: 0.6981 - accuracy: 0.8024 - val_loss: 0.3347 - val_accuracy: 0.8527\n",
      "Epoch 2/6\n",
      "4535/4535 [==============================] - 79s 17ms/step - loss: 0.1136 - accuracy: 0.9585 - val_loss: 0.3568 - val_accuracy: 0.8536\n",
      "Epoch 3/6\n",
      "4535/4535 [==============================] - 75s 17ms/step - loss: 0.0349 - accuracy: 0.9892 - val_loss: 0.4358 - val_accuracy: 0.8580\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=maxlen, trainable=False))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(512, activation=\"relu\"))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(Dense(1, activation=\"sigmoid\"))\n",
    "model2.summary()\n",
    "model2.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "history = model2.fit(x_train, y_train, validation_split=0.2, epochs=6, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe28621a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.8809523582458496\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model2.evaluate(x_test, y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f634a5f",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de66b624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 1000, 100)         4255200   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1000, 256)         77056     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 334, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 334, 32)           24608     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 10688)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10688)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 10688)             42752     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               2736384   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 7,136,257\n",
      "Trainable params: 2,859,681\n",
      "Non-trainable params: 4,276,576\n",
      "_________________________________________________________________\n",
      "Train on 4535 samples, validate on 1134 samples\n",
      "Epoch 1/6\n",
      "4535/4535 [==============================] - 104s 23ms/step - loss: 0.5610 - accuracy: 0.7839 - val_loss: 0.3537 - val_accuracy: 0.8563\n",
      "Epoch 2/6\n",
      "4535/4535 [==============================] - 103s 23ms/step - loss: 0.3113 - accuracy: 0.8650 - val_loss: 0.3908 - val_accuracy: 0.8404\n",
      "Epoch 3/6\n",
      "4535/4535 [==============================] - 103s 23ms/step - loss: 0.2594 - accuracy: 0.8891 - val_loss: 0.3509 - val_accuracy: 0.8695\n",
      "Epoch 4/6\n",
      "4535/4535 [==============================] - 103s 23ms/step - loss: 0.2251 - accuracy: 0.9085 - val_loss: 0.4794 - val_accuracy: 0.8342\n",
      "Epoch 5/6\n",
      "4535/4535 [==============================] - 103s 23ms/step - loss: 0.2079 - accuracy: 0.9162 - val_loss: 0.3506 - val_accuracy: 0.8695\n",
      "Epoch 6/6\n",
      "4535/4535 [==============================] - 102s 23ms/step - loss: 0.2208 - accuracy: 0.9107 - val_loss: 0.6064 - val_accuracy: 0.7937\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=maxlen, trainable=False))\n",
    "model3.add(Conv1D(256, 3, padding='same', activation='relu'))\n",
    "model3.add(MaxPool1D(3, 3, padding='same'))\n",
    "model3.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dropout(0.3))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dense(256, activation='relu'))\n",
    "model3.add(Dropout(0.2))\n",
    "model3.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model3.summary()\n",
    "model3.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "history = model3.fit(x_train, y_train, validation_split=0.2, epochs=6, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f9579a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.8095238208770752\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model3.evaluate(x_test, y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944d8a75",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7acc9036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=3, random_state=None, shuffle=False)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3)\n",
    "kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9b70dae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3359 samples, validate on 840 samples\n",
      "Epoch 1/6\n",
      "3359/3359 [==============================] - 77s 23ms/step - loss: 0.5452 - accuracy: 0.7797 - val_loss: 0.4047 - val_accuracy: 0.8405\n",
      "Epoch 2/6\n",
      "3359/3359 [==============================] - 76s 23ms/step - loss: 0.3134 - accuracy: 0.8684 - val_loss: 2.0377 - val_accuracy: 0.6321\n",
      "Epoch 3/6\n",
      "3359/3359 [==============================] - 75s 22ms/step - loss: 0.2843 - accuracy: 0.8872 - val_loss: 0.4297 - val_accuracy: 0.8631\n",
      "Train on 3359 samples, validate on 840 samples\n",
      "Epoch 1/6\n",
      "3359/3359 [==============================] - 78s 23ms/step - loss: 0.5659 - accuracy: 0.7597 - val_loss: 0.4618 - val_accuracy: 0.8369\n",
      "Epoch 2/6\n",
      "3359/3359 [==============================] - 76s 22ms/step - loss: 0.3335 - accuracy: 0.8634 - val_loss: 0.8504 - val_accuracy: 0.7917\n",
      "Epoch 3/6\n",
      "3359/3359 [==============================] - 76s 23ms/step - loss: 0.3465 - accuracy: 0.8634 - val_loss: 0.4956 - val_accuracy: 0.8048\n",
      "Train on 3360 samples, validate on 840 samples\n",
      "Epoch 1/6\n",
      "3360/3360 [==============================] - 79s 23ms/step - loss: 0.5048 - accuracy: 0.7926 - val_loss: 0.4176 - val_accuracy: 0.8131\n",
      "Epoch 2/6\n",
      "3360/3360 [==============================] - 76s 23ms/step - loss: 0.3097 - accuracy: 0.8720 - val_loss: 0.3773 - val_accuracy: 0.8417\n",
      "Epoch 3/6\n",
      "3360/3360 [==============================] - 76s 23ms/step - loss: 0.2602 - accuracy: 0.9012 - val_loss: 0.3320 - val_accuracy: 0.8786\n",
      "Epoch 4/6\n",
      "3360/3360 [==============================] - 76s 23ms/step - loss: 0.2432 - accuracy: 0.9060 - val_loss: 0.5440 - val_accuracy: 0.7893\n",
      "Epoch 5/6\n",
      "3360/3360 [==============================] - 78s 23ms/step - loss: 0.1987 - accuracy: 0.9214 - val_loss: 0.4465 - val_accuracy: 0.8429\n",
      "[0.8780952095985413, 0.8780952095985413, 0.8089566230773926]\n"
     ]
    }
   ],
   "source": [
    "L = []\n",
    "for train_index, test_index in kf.split(features_data, labels_data):\n",
    "    x_train, x_test, y_train, y_test = features_data[train_index], features_data[test_index], labels_data[train_index], labels_data[test_index]\n",
    "    \n",
    "    model4 = Sequential()\n",
    "\n",
    "    model4.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=maxlen, trainable=False))\n",
    "    model4.add(Conv1D(256, 3, padding='same', activation='relu'))\n",
    "    model4.add(MaxPool1D(3, 3, padding='same'))\n",
    "    model4.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    model4.add(Flatten())\n",
    "    model4.add(Dropout(0.3))\n",
    "    model4.add(BatchNormalization())\n",
    "    model4.add(Dense(256, activation='relu'))\n",
    "    model4.add(Dropout(0.2))\n",
    "    model4.add(Dense(units=1, activation='sigmoid'))\n",
    "    \n",
    "    model4.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "    \n",
    "    model4.fit(x_train, y_train, epochs=6, callbacks=[early_stop], validation_split=0.2)\n",
    "    \n",
    "    test_loss, test_acc = model4.evaluate(x_test, y_test, verbose=2)\n",
    "    \n",
    "    L.append(test_acc)  \n",
    "    \n",
    "print(L)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c54ea8c",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd5967",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_LSTM_text.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.7",
   "language": "python",
   "name": "pythonproject1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
